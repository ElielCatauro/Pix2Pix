{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "pix2pix.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JavierMoralesEstevez/Pix2Pix/blob/master/pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LduyW5gj56As"
      },
      "source": [
        "# Presentación\n",
        "Hola DotCSV e interesados en el machine learning, Os presento mi implementación del algoritmo **pix2pix**.\n",
        "\n",
        "En mi caso lo he rebautizado como **voice2voice**, ya que utilizo audios en lugar de imágenes, audios que transformo a imágenes para posteriormente procesarlas en la red neuronal pix2pix.\n",
        "\n",
        "Tengo que decir que me he topado con varios problemas. Ya que los audios \"Target\" pueden ocupar más bytes que los de una imagen de 256x256 pixels, aún teniendo en cuenta los tres canales de color, he tenido que intentar bajar la calidad de los audios, sin resultados cualitativos y finalmente utilizar imágenes de 512x512 pixels. Con todo lo que conlleva, más procesamiento y como se me hacía tarde para la entrega, procesamiento en la nube de Google Cloud (sponsor) con una tarjeta Nvidia Tesla P100.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OrEL_zaFRBVs",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyU4xYsAper7",
        "colab_type": "text"
      },
      "source": [
        "Para el procesamiento en la nube, tuve de comentar esta línea:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-ZMlvV8nYIhd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2710fd4e-0b37-403d-9438-ff78038c5ee9"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t_g_kShp3u7",
        "colab_type": "text"
      },
      "source": [
        "Definimos los imports y las rutas.\n",
        "\n",
        "Para crear los inputs me descargué el videotutorial de DotCSV sobre la implementación de pix2pix, en forma de audio. Conjuntamente con la transcripción de youtube.\n",
        "\n",
        "Utilicé ffmpeg para, con el tiempo de las frases, recortar los audios en trozos mas pequeños. También generé esa misma frase con google translate y me descargué el audio. Dejo una pequeña aplicación en php para la descarga de los audios de google translate.\n",
        "\n",
        "La transcripción de youtube podría haber sido mejor, pero no pensé opciones en ese momento. También hice una criba de los archivos de mayor tamaño.\n",
        "\n",
        "Trabajaremos con una versión de archivos más reducida que la producida originalmente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pe0uC48GRgcs",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ruta raiz\n",
        "PATH = \"/content/drive/My Drive/\"\n",
        "#PATH = \"Voices/\"\n",
        "\n",
        "# ruta de datos de entrada\n",
        "INaudio =  PATH + \"inputVoices/\"\n",
        "TARaudio =  PATH + \"targetVoices/\"\n",
        "# ruta de datos de salida\n",
        "OUTaudio =  PATH + \"outputVoices/\"\n",
        "# ruta de los checkpoints\n",
        "CKPATH = PATH + \"checkpointsVoices/\"\n",
        "\n",
        "# estas carpetas contendrán los audios y las imagenes con la evolución\n",
        "#OUTaudioAux =  PATH + \"outputVoicesAux/\"\n",
        "#INaudioAux =  PATH + \"inputVoicesAux/\"\n",
        "#TARaudioAux =  PATH + \"targetVoicesAux/\"\n",
        "#INaudioJpeg =  PATH + \"inputVoicesJpeg/\"\n",
        "#TARaudioJpeg =  PATH + \"targetVoicesJpeg/\"\n",
        "#OUTaudioJpeg =  PATH + \"outputVoicesJpeg/\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBdjdpHDp77T",
        "colab_type": "text"
      },
      "source": [
        "Leemos el directorio y guardamos los inputs en variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jRBF7Ag4buI6",
        "colab": {}
      },
      "source": [
        "\n",
        "imgurls = !ls -1 \"{TARaudio}\"\n",
        "\n",
        "n = len(imgurls)\n",
        "\n",
        "train_n = round(n * 0.80)\n",
        "\n",
        "# listado randomizado\n",
        "randurls = np.copy(imgurls)\n",
        "\n",
        "#np.random.seed(23) # solo para el tutorial\n",
        "np.random.shuffle(randurls)\n",
        "\n",
        "# partición train/test\n",
        "tr_urls = randurls[:train_n]\n",
        "ts_urls = randurls[train_n:n]\n",
        "\n",
        "print(len(imgurls), len(tr_urls), len(ts_urls))\n",
        "tr_urls\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkdvSWm9uuy9",
        "colab_type": "text"
      },
      "source": [
        "Seleccionamos el tamaño de las imágenes y procedemos a redimensionar el audio para que parezca una imagen.\n",
        "\n",
        "Una imagen 256x256 pixeles en jpeg no es más que un array de 256x256x3, 3 canales de color, que por supuesto utilizo para guardar más datos del audio.\n",
        "Como he comentado anteriormente utilizo tamaños de 512x512x3.\n",
        "\n",
        "Es una lastima que tensorflow no procese imagenes .tiff, pues las necesitamos ya que el audio contiene decimales (float) y los jpeg no."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ouaO0YULUB3t",
        "colab": {}
      },
      "source": [
        "IMG_WIDTH = 512\n",
        "IMG_HEIGHT = 512\n",
        "\n",
        "\n",
        "# reescalar imagenes\n",
        "def resize(inimg, tgimg, width, height):\n",
        "    inimg = tf.image.resize(inimg, (width, height) )\n",
        "    tgimg = tf.image.resize(tgimg, (width, height) )\n",
        "    return inimg, tgimg\n",
        "\n",
        "\n",
        "# reescalar imagenes\n",
        "def resize1D(inimg, tgimg, width, height):\n",
        "    n=width*height*3\n",
        "    n1 = n - tf.shape(inimg)[0]*1\n",
        "    n2 = n - tf.shape(tgimg)[0]*2\n",
        "    inimg = tf.reshape(inimg, [tf.shape(inimg)[0]*tf.shape(inimg)[1],1] )\n",
        "    tgimg = tf.reshape(tgimg, [tf.shape(tgimg)[0]*tf.shape(tgimg)[1],1] )\n",
        "    inimg = tf.pad(inimg, ([n1,0],[0,0]), mode='CONSTANT')\n",
        "    tgimg = tf.pad(tgimg, ([n2,0],[0,0]), mode='CONSTANT')\n",
        "    return inimg, tgimg\n",
        "\n",
        "\n",
        "#restaurar dimensiones desde 3dim a 1dim\n",
        "#@tf.function()\n",
        "def resize3to1(imagen, c):\n",
        "    mask = tf.math.not_equal(imagen, 0)\n",
        "    imagen = tf.boolean_mask(imagen, mask)\n",
        "    dim = tf.cast((tf.shape(imagen)[0]/c), tf.int32)\n",
        "    imagen = tf.reshape(imagen, (dim,c) )\n",
        "    return imagen\n",
        "\n",
        "\n",
        "# reescalar imagenes desde 1dim(Zx1) a 3dim\n",
        "def resize1to3(inimg, tgimg):\n",
        "    a = tf.cast(tf.shape(inimg)[0], tf.float32)\n",
        "    b = tf.cast(tf.shape(tgimg)[0], tf.float32)\n",
        "    n1 = tf.cast((tf.math.sqrt(a/3)), tf.int32)\n",
        "    n2 = tf.cast((tf.math.sqrt((b/3))),tf.int32)\n",
        "    inimg = tf.reshape(inimg, (n1,n1,3) )\n",
        "    tgimg = tf.reshape(tgimg, (n2,n2,3) )\n",
        "    return inimg, tgimg\n",
        "\n",
        "\n",
        "def normalize(inimg, tgimg):\n",
        "    #inimg = inimg[:,:,-1]\n",
        "    #tgimg = tgimg[:,:,-1]\n",
        "    inimg = (inimg / 127.5) -1\n",
        "    tgimg = (tgimg / 127.5) -1\n",
        "\n",
        "    return inimg, tgimg\n",
        "\n",
        "\n",
        "# aumentación de datos: random crop + flip\n",
        "@tf.function()\n",
        "def random_jitter(inimg, tgimg):\n",
        "  \n",
        "    inimg, tgimg = resize(inimg, tgimg, (IMG_WIDTH+20), (IMG_HEIGHT+20))\n",
        "  \n",
        "    stacked_image = tf.stack([inimg, tgimg], axis=0)\n",
        "    cropped_image = tf.image.random_crop(stacked_image, [2, IMG_WIDTH, IMG_HEIGHT, 3 ])\n",
        "    inimg, tgimg = cropped_image[0], cropped_image[1]\n",
        "  \n",
        "    if np.random.uniform() > 0.5: \n",
        "        #inimg = tf.image.flip_up_down(inimg)\n",
        "        #tgimg = tf.image.flip_up_down(tgimg)\n",
        "        inimg = tf.image.flip_left_right(inimg)\n",
        "        tgimg = tf.image.flip_left_right(tgimg)\n",
        "    \n",
        "    return inimg, tgimg\n",
        "\n",
        "\n",
        "def save_audio(imagen, filename):\n",
        "    #img1 = tf.io.encode_jpeg(tf.cast(imagen, tf.uint8 ))\n",
        "    #tf.io.write_file(OUTaudioJpeg+filename+\".jpeg\", img1  )\n",
        "    imagen = resize3to1(imagen, 1)\n",
        "    imagen = tf.audio.encode_wav(imagen, 88200)\n",
        "    tf.io.write_file(OUTaudio+filename+\".wav\", imagen)\n",
        "    \n",
        "\n",
        "#@tf.function()\n",
        "def openImage(filename):\n",
        "\n",
        "    inimg, samplerate1 = tf.audio.decode_wav(tf.io.read_file(INaudio+filename))\n",
        "    tgimg, samplerate2 = tf.audio.decode_wav(tf.io.read_file(TARaudio+filename))\n",
        "  \n",
        "    inimg, tgimg = resize1D(inimg, tgimg, IMG_WIDTH, IMG_HEIGHT )\n",
        "    inimg, tgimg = resize1to3(inimg, tgimg)\n",
        "    return inimg, tgimg\n",
        "  \n",
        "\n",
        "#@tf.function()\n",
        "def load_image(filename, augment= True):\n",
        "\n",
        "    #inimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(inputs+filename)), tf.float32)[...,:3]\n",
        "    #tgimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(inputs+filename)), tf.float32)[...,:3]\n",
        "  \n",
        "    inimg, tgimg = openImage(filename)\n",
        "\n",
        "    if augment:\n",
        "        inimg, tgimg = random_jitter(inimg, tgimg )\n",
        "  \n",
        "    #inimg, tgimg = normalize(inimg, tgimg)\n",
        "    \n",
        "    return inimg, tgimg\n",
        "\n",
        "\n",
        "\n",
        "def load_train_image(filename):\n",
        "    return load_image(filename, True)\n",
        "\n",
        "\n",
        "def load_test_image(filename):\n",
        "    return load_image(filename, False)\n",
        "\n",
        "\n",
        "print(randurls[0])\n",
        "#plt.imshow(load_train_image(randurls[0]))\n",
        "#load_train_image(randurls[0])\n",
        "plt.imshow(((load_train_image(randurls[0])[1]) + 1) / 2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stgIZMSCwekr",
        "colab_type": "text"
      },
      "source": [
        "El código a continuación no cambia del original, crearemos un dataset con los datos de entrada, implementaremos las funciones necesarias para crear la red neuronal pix2pix y finalmente la entrenaremos no sin antes guardar los inputs y outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_kqV5YC1ZR6G",
        "colab": {}
      },
      "source": [
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(tr_urls)\n",
        "train_dataset = train_dataset.map(load_train_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(1)\n",
        "\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(tr_urls)\n",
        "test_dataset = test_dataset.map(load_test_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(1)\n",
        "\n",
        "\n",
        "for inimg, tgimg in train_dataset.take(5):\n",
        "    plt.imshow(((inimg[0,...]) + 1) / 2)\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "47No6Vadapt4",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "def downsample(filters, apply_batchnorm=True):\n",
        "\n",
        "    result = Sequential()\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0, 0.02)\n",
        "\n",
        "    # capa convolucional\n",
        "    result.add(Conv2D(filters,\n",
        "                    kernel_size=4,\n",
        "                    strides=2,\n",
        "                    padding=\"same\",\n",
        "                    kernel_initializer=initializer,\n",
        "                    use_bias=not apply_batchnorm))\n",
        "  \n",
        "    if apply_batchnorm:\n",
        "      # capa batchNorm.\n",
        "      result.add(BatchNormalization())\n",
        "\n",
        "    # capa de activación\n",
        "    result.add(LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "downsample(64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b5LgrqQ-zTTz",
        "colab": {}
      },
      "source": [
        "def upsample(filters, apply_dropout=False):\n",
        "\n",
        "    result = Sequential()\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0, 0.02)\n",
        "\n",
        "    # capa convolucional\n",
        "    result.add(Conv2DTranspose(filters,\n",
        "                            kernel_size=4,\n",
        "                            strides=2,\n",
        "                            padding=\"same\",\n",
        "                            kernel_initializer=initializer,\n",
        "                            use_bias=False))\n",
        "  \n",
        "    # capa batchNorm.\n",
        "    result.add(BatchNormalization())\n",
        "\n",
        "    if apply_dropout:\n",
        "      # capa de Dropout\n",
        "      result.add(Dropout(0.5))\n",
        "\n",
        "    # capa de activación\n",
        "    result.add(ReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "upsample(64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tdRVI4dI0F-k",
        "colab": {}
      },
      "source": [
        "def Generator():\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=[None,None,3])\n",
        "\n",
        "    down_stack = [\n",
        "                downsample(64, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "                downsample(128),                        # (bs, 64,  64,  128)\n",
        "                downsample(256),\n",
        "                downsample(512),\n",
        "                downsample(512),\n",
        "                downsample(512),\n",
        "                downsample(512),\n",
        "                downsample(512),\n",
        "    ]\n",
        "\n",
        "    up_stack = [\n",
        "              upsample(512, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "              upsample(512, apply_dropout=True),\n",
        "              upsample(512, apply_dropout=True),\n",
        "              upsample(512),\n",
        "              upsample(256),\n",
        "              upsample(128),\n",
        "              upsample(64),\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0, 0.02)\n",
        "\n",
        "    last = Conv2DTranspose(filters = 3, \n",
        "                         kernel_size=4,\n",
        "                         strides = 2,\n",
        "                         padding = \"same\",\n",
        "                         kernel_initializer = initializer,\n",
        "                         activation = \"tanh\")\n",
        "  \n",
        "    x = inputs\n",
        "    s = []\n",
        "\n",
        "    concat = Concatenate()\n",
        "\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        s.append(x)\n",
        "\n",
        "    s = reversed(s[:-1])\n",
        "\n",
        "    for up, sk in zip(up_stack, s):\n",
        "        s\n",
        "        x = up(x)\n",
        "        x = concat([x, sk])\n",
        "\n",
        "    last = last(x)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=last)\n",
        "\n",
        "generator = Generator()\n",
        "#inimg = tf.shape(tf.expand_dims(inimg, 0))\n",
        "#gen_output = generator(((inimg+1)*255), training=False)\n",
        "#plt.imshow(gen_output[0,...])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jrKLq2aWQ_XE",
        "colab": {}
      },
      "source": [
        "def Discriminator():\n",
        "\n",
        "    ini = Input(shape=[None, None, 3], name=\"input_img\")\n",
        "    gen = Input(shape=[None, None, 3], name=\"gener_img\")\n",
        "\n",
        "    con = concatenate([ini, gen])\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0, 0.02)\n",
        "\n",
        "    down1 =  downsample(64, apply_batchnorm=False)(con)\n",
        "    down2 =  downsample(128)(down1)\n",
        "    down3 =  downsample(256)(down2)\n",
        "    down4 =  downsample(512)(down3)\n",
        "\n",
        "    last = tf.keras.layers.Conv2D(filters = 1, \n",
        "                                kernel_size=4,\n",
        "                                strides = 1,\n",
        "                                kernel_initializer = initializer,\n",
        "                                padding = \"same\",)(down4)\n",
        "    return tf.keras.Model(inputs=[ini, gen], outputs=last)\n",
        "\n",
        "discriminator = Discriminator()\n",
        "\n",
        "#disc_out = discriminator([((inimg+1)*255), gen_output], training=False)\n",
        "#plt.imshow(disc_out[0,...,-1], vmin=-20, vmax=20, cmap=\"RdBu_r\")\n",
        "#plt.colorbar()\n",
        "#disc_out.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zAnuwNPlWi-C",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fbuNJkVtWyUW",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    # diferencia entre los true por ser real y el detectado por el discriminador\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "    # diferencia entre los false por ser generado y el detectado por el discriminador\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "    return total_disc_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "joZe5NfsYBgY",
        "colab": {}
      },
      "source": [
        "LAMBDA = 100\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    # mean absolute error\n",
        "    li_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "    total_gen_loss = gan_loss + (LAMBDA * li_loss)\n",
        "\n",
        "    return total_gen_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZN1jSoBEZM-T",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "generator_optimizer     = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "checkpoint_prefix = os.path.join(CKPATH, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(CKPATH))\n",
        "#checkpoint.restore(tf.train.latest_checkpoint(CKPATH)).assert_consumed()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Li_2VhxEb9",
        "colab_type": "text"
      },
      "source": [
        "En la siguiente función (generate_images) he incorporado una llamada a las funciones encargadas de guardar las imagenes y los audios de los inputs y outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c1zAbC-GaHwv",
        "colab": {}
      },
      "source": [
        "def generate_images(model, test_input, tar, save_filename=False, display_imgs=True):\n",
        "  \n",
        "    prediction = model(test_input, training=True)\n",
        "\n",
        "    #if save_filename:\n",
        "        #tf.keras.preprocessing.image.save_img(OUTaudioAux + save_filename + \".jpg\", prediction[0,...])\n",
        "        #save_audio(prediction[0,...], save_filename) #guarda el audio e imagen de la predicción\n",
        "    \n",
        "    plt.figure(figsize=(10,10))\n",
        "    \n",
        "    display_list = [test_input[0], tar[0], prediction[0]]\n",
        "    title = [\"Input image\", \"Ground truth\", \"Predicted image\"]\n",
        "\n",
        "    if display_imgs:\n",
        "        for i in range(3):\n",
        "            plt.subplot(1, 3, i+1)\n",
        "            plt.title(title[i])\n",
        "            # getting the pixel values between [0,1] to plot it.\n",
        "            plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AHyOONwGc02p",
        "colab": {}
      },
      "source": [
        "@tf.function()\n",
        "def train_step(input_image, target):\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as discr_tape:\n",
        "\n",
        "        output_image = generator(input_image, training= True)\n",
        "\n",
        "        output_gen_discr = discriminator([output_image, input_image], training=True)\n",
        "\n",
        "        output_trg_discr = discriminator([target, input_image], training=True)\n",
        "\n",
        "        discr_loss= discriminator_loss(output_trg_discr, output_gen_discr)\n",
        "\n",
        "        gen_loss = generator_loss(output_gen_discr, output_image, target)\n",
        "\n",
        "\n",
        "        generator_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "\n",
        "        discriminator_grads = discr_tape.gradient(discr_loss, discriminator.trainable_variables)\n",
        "\n",
        "        generator_optimizer.apply_gradients(zip(generator_grads, generator.trainable_variables))\n",
        "\n",
        "        discriminator_optimizer.apply_gradients(zip(discriminator_grads, discriminator.trainable_variables))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nnx_hS51bbHm",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        imgi = 0\n",
        "        for input_image, target in dataset:\n",
        "            print(\"epoch \" + str(epoch) + \" - train: \" + str(imgi) + \"/\" + str(len(tr_urls)))\n",
        "            imgi += 1\n",
        "            train_step(input_image, target)\n",
        "            clear_output(wait=True)\n",
        "\n",
        "        imgi = 0\n",
        "        #clear_output(wait=True)\n",
        "        for inp, tar in test_dataset.take(5):\n",
        "            generate_images(generator, inp, tar, str(imgi) + \"_\" + str(epoch), display_imgs=True)\n",
        "            imgi += 1\n",
        "\n",
        "        # saving (checkpoint) the model every 10 epochs\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QV6c-F8wfk2U",
        "colab": {}
      },
      "source": [
        "train(train_dataset, 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCEBU6CeG6VH",
        "colab_type": "text"
      },
      "source": [
        "POSTDATA: En la carpeta \"mivoice\" de github hay un programa escrito en php para descargar los inputs de la red neuronal, tan solo debeis escribir el contenido de la pequeña frase en index.html y se os descargará un archivo.wav"
      ]
    }
  ]
}